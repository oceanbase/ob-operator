---
sidebar_position: 2
---

# 故障恢复

本文介绍如何使用 ob-operator 对于 OceanBase 节点故障进行自动恢复。

:::note

在 OceanBase 4.2.3.0 版本之前内核无法采用虚拟 IP 进行通信，当 Pod IP 发生变化时 observer 将无法正常启动，需要采用某种方式将节点的 IP 固定住才能够让 observer 失活后原地重启。否则只能依赖多数派存活的条件将新的 Pod 以新的节点的方式加入集群并同步数据来恢复原来的节点数量。

:::

## 基于 OceanBase 多副本能力

### 注意事项

* 要成功恢复 OceanBase 集群，需要部署至少三个节点，并且租户也是至少三个副本。
* **只能应对少数派节点失活的情况**，例如 3 个节点的集群中只能容忍 1 个节点失活。

### 恢复策略

节点失活后 ob-operator 会自动检测到该情况，并且创建出新的 Pod 作为“新节点”加入到原来的集群当中。加入集群之后新节点会向原有节点同步数据，直到所有数据同步完成。

需要注意的是，在恢复过程中如果多数派失活，那么集群将无法恢复。如果出现这种情况，需要从备份中手动恢复数据。

## 基于网络插件 Calico

### 注意事项

* 若要使用保留 Pod IP 地址恢复的能力，需要集群使用 [Calico](https://docs.tigera.io/calico/latest/getting-started/kubernetes/) 作为网络插件。

### 恢复策略

对于少数派故障，OceanBase 凭借多副本机制还能保证集群可用，这时 ob-operator 会发现 pod 异常，然后通过 add server 再 delete server 的方式，新建一个新的 observer 加入到集群，再删除原来的 observer, OceanBase 会自动利用新加入的 observer 去补全数据副本。

如果 K8s 集群使用了 calico 网络插件，那么这个过程将更加容易，ob-operator 会通过指定 ip 的形式，指定原 IP 地址来启动一个新的 observer，这样如果数据还存在的话，会直接利用原 server 的数据，并不需要再重新复制一份数据，而且对于多数派故障，这种方式也能在新的 observer 都启动了之后恢复服务。

## 基于 Kubernetes Service

### 注意事项

- 仅 OceanBase 数据库内核版本 >= 4.2.3.0 支持该特性。
- 创建 OBCluster 时，需要配置 `service` 模式。配置片段如下：

```yaml
apiVersion: oceanbase.oceanbase.com/v1alpha1
kind: OBCluster
metadata:
  name: test
  namespace: oceanbase
  annotations:
    oceanbase.oceanbase.com/mode: service # 这里是关键配置
spec:
# ...
```

### 恢复策略

创建 service 模式的 OBCluster 时，ob-operator 会为每个 OBServer pod 附加一个 `service`，并将该 `service` 的 ClusterIP 作为网络通信 IP。

1. 当 OBServer pod 重启时，因为采用恒定 service 的 ClusterIP 作为通信 IP，observer 能够实现原地重启；
2. 当 OBServer pod 被误删时，ob-operator 会创建一个新的 OBServer pod 并使用相同的 ClusterIP 进行通信，新的节点将自动加入 OceanBase 集群并恢复工作。

## 验证

您可以通过以下方式验证 ob-operator 的故障恢复能力：

1. 删除 pod，例如删除 zone1 的 pod。

```shell
kubectl delete pod obcluster-1-zone1-074bda77c272 -n oceanbase
```

2. 查看恢复情况, 可以看到 zone1 对应的 pod 已经被新建出来，并且Ready。

```shell
kubectl get pods -n oceanbase

NAME                                  READY   STATUS    RESTARTS   AGE
obcluster-1-zone3-074bda77c272        2/2     Running   0          12d
obcluster-1-zone2-7ecbd89f84de        2/2     Running   0          12d
obcluster-1-zone1-94ecf05cb290        2/2     Running   0          1m
```

## 部署建议

基于以上的介绍，如果您要部署生产使用的集群，要得到较好的故障恢复能力的话，推荐使用 calico 网络插件（或者在 ob-operator 2.2.0 及之后的版本中配置集群为 `service` 模式），并且集群至少部署 3 节点，租户副本数量至少 3 个，每个 zone 的节点选择尽量在不同的机器上，以便尽可能的降低整个集群故障无法恢复的可能，另外，ob-operator 还提供了基于[备份恢复](500.data-recovery-of-ob-operator.md)和[主备租户](400.tenant-backup-of-ob-operator.md)的高可用方案，可以参考对应章节。
