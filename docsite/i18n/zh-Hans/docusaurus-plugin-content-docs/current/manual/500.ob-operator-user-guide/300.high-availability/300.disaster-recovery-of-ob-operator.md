# 故障恢复

本文介绍 ob-operator 对于 OceanBase 节点故障的处理

## 前提条件

* 要成功恢复 OceanBase 集群，需要部署至少三个节点，并且租户也是至少三个副本。
* 若要使用保留 IP 地址恢复的能力，需要集群使用 calico 作为网络插件。

## 恢复策略

对于少数派故障，OceanBase 凭借多副本机制还能保证集群可用，这时 ob-operator 会发现 pod 异常，然后通过 add server 再 delete server 的方式，新建一个新的 observer 加入到集群，再删除原来的 observer
, OceanBase 会自动利用新加入的 observer 去补全数据副本。

如果 K8s 集群使用了 calico 网络插件，那么这个过程将更加容易，ob-operator 会通过指定 ip 的形式，指定原 IP 地址来启动一个新的 observer，这样如果数据还存在的话，会直接利用原 server 的数据，并不需要再重新复制一份数据，而且对于多数派故障，这种方式也能在新的 observer 都启动了之后恢复服务。

## 验证

您可以通过以下方式验证 ob-operator 的故障恢复能力

删除 pod

```shell
kubectl delete pod obcluster-1-zone1-074bda77c272 -n oceanbase
```

查看恢复情况, 可以看到 zone1 对应的 pod 已经被新建出来，并且Ready。

```shell
kubectl get pods -n oceanbase

NAME                                  READY   STATUS    RESTARTS   AGE
obcluster-1-zone3-074bda77c272        2/2     Running   0          12d
obcluster-1-zone2-7ecbd89f84de        2/2     Running   0          12d
obcluster-1-zone1-94ecf05cb290        2/2     Running   0          1m
```

## 部署建议

基于以上的介绍，如果您要部署生产使用的集群，要得到较好的故障恢复能力的话，推荐使用 calico 网络插件，并且集群至少部署 3 节点，租户副本数量至少 3 个，每个 zone 的节点选择尽量在不同的机器上，以便尽可能的降低整个集群故障无法恢复的可能，另外，ob-operator 还提供了基于[备份恢复](500.data-recovery-of-ob-operator.md)和[主备租户](400.tenant-backup-of-ob-operator.md)的高可用方案，可以参考对应章节。
